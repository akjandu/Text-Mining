{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    " import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isinbayeva claims new world best</td>\n",
       "      <td>Pole vaulter Yelena Isinbayeva broke her own i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smith aims to bring back respect</td>\n",
       "      <td>Scotland manager Walter Smith says he wants to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chancellor rallies Labour voters</td>\n",
       "      <td>Gordon Brown has issued a rallying cry to supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaders meet over Turkish EU bid</td>\n",
       "      <td>Tony Blair has met Italian Prime Minister Silv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Davenport dismantles young rival</td>\n",
       "      <td>Top seed Lindsay Davenport booked her place in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary  \\\n",
       "0  Isinbayeva claims new world best   \n",
       "1  Smith aims to bring back respect   \n",
       "2  Chancellor rallies Labour voters   \n",
       "3  Leaders meet over Turkish EU bid   \n",
       "4  Davenport dismantles young rival   \n",
       "\n",
       "                                                Text  \n",
       "0  Pole vaulter Yelena Isinbayeva broke her own i...  \n",
       "1  Scotland manager Walter Smith says he wants to...  \n",
       "2  Gordon Brown has issued a rallying cry to supp...  \n",
       "3  Tony Blair has met Italian Prime Minister Silv...  \n",
       "4  Top seed Lindsay Davenport booked her place in...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"C:\\\\Users\\\\beqaj\\\\Desktop\\\\NLP Lab\\\\project\\\\news.csv\")\n",
    "news.shape\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    0\n",
       "Text       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isinbayeva claims new world best</td>\n",
       "      <td>Pole vaulter Yelena Isinbayeva broke her own i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smith aims to bring back respect</td>\n",
       "      <td>Scotland manager Walter Smith says he wants to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chancellor rallies Labour voters</td>\n",
       "      <td>Gordon Brown has issued a rallying cry to supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaders meet over Turkish EU bid</td>\n",
       "      <td>Tony Blair has met Italian Prime Minister Silv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Davenport dismantles young rival</td>\n",
       "      <td>Top seed Lindsay Davenport booked her place in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Summary  \\\n",
       "0  Isinbayeva claims new world best   \n",
       "1  Smith aims to bring back respect   \n",
       "2  Chancellor rallies Labour voters   \n",
       "3  Leaders meet over Turkish EU bid   \n",
       "4  Davenport dismantles young rival   \n",
       "\n",
       "                                                Text  \n",
       "0  Pole vaulter Yelena Isinbayeva broke her own i...  \n",
       "1  Scotland manager Walter Smith says he wants to...  \n",
       "2  Gordon Brown has issued a rallying cry to supp...  \n",
       "3  Tony Blair has met Italian Prime Minister Silv...  \n",
       "4  Top seed Lindsay Davenport booked her place in...  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News # 1\n",
      "Isinbayeva claims new world best\n",
      "Pole vaulter Yelena Isinbayeva broke her own indoor world record by clearing 4.89 metres in Lievin on Saturday.\\n\\nIt was the Russian\\'s 12th world record of her career and came just a few days after she cleared 4.88m at the Norwich Union Grand Prix in Birmingham. The Olympic champion went on to attempt 5.05m at the meeting on France but failed to clear that height. In the men\\'s 60m\n",
      "\n",
      "News # 2\n",
      "Smith aims to bring back respect\n",
      "Scotland manager Walter Smith says he wants to restore the national team\\'s respectability in world football.\\n\\nSmith has joined his first squad for a three-day get-together near Manchester in preference to playing a friendly. While qualification for the 2006 World Cup appears to be beyond Scotland\n",
      "\n",
      "News # 3\n",
      "Chancellor rallies Labour voters\n",
      "Gordon Brown has issued a rallying cry to supporters\n",
      "\n",
      "News # 4\n",
      "Leaders meet over Turkish EU bid\n",
      "Tony Blair has met Italian Prime Minister Silvio Berlusconi and German Chancellor Gerhard Schroeder to talk about Turkey entering the EU.\\n\\nThe Downing Street talks covered a range of other topics ahead of an EU summit in Brussels later in the week. Mr Blair is an enthusiastic proponent of talks to bring Turkey within the recently-expanded EU. Italy and Germany also favour an early start to talks\n",
      "\n",
      "News # 5\n",
      "Davenport dismantles young rival\n",
      "Top seed Lindsay Davenport booked her place in the last 16 of the Australian Open with a convincing 6-2 6-4 win over Nicole Vaidisova of the Czech Republic.\\n\\nThe American had too much power for her 15-year-old opponent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the news\n",
    "for i in range(5):\n",
    "    print(\"News #\",i+1)\n",
    "    print(news.Summary[i])\n",
    "    print(news.Text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for summary in news.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in news.Text:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean News # 1\n",
      "isinbayeva claims new world best\n",
      "pole vaulter yelena isinbayeva broke indoor world record clearing 4 89 metres lievin saturday \\n\\nit russian\\ 12th world record career came days cleared 4 88m norwich union grand prix birmingham olympic champion went attempt 5 05m meeting france failed clear height men\\ 60m\n",
      "\n",
      "Clean News # 2\n",
      "smith aims to bring back respect\n",
      "scotland manager walter smith says wants restore national team\\ respectability world football \\n\\nsmith joined first squad three day get together near manchester preference playing friendly qualification 2006 world cup appears beyond scotland\n",
      "\n",
      "Clean News # 3\n",
      "chancellor rallies labour voters\n",
      "gordon brown issued rallying cry supporters\n",
      "\n",
      "Clean News # 4\n",
      "leaders meet over turkish eu bid\n",
      "tony blair met italian prime minister silvio berlusconi german chancellor gerhard schroeder talk turkey entering eu \\n\\nthe downing street talks covered range topics ahead eu summit brussels later week mr blair enthusiastic proponent talks bring turkey within recently expanded eu italy germany also favour early start talks\n",
      "\n",
      "Clean News # 5\n",
      "davenport dismantles young rival\n",
      "top seed lindsay davenport booked place last 16 australian open convincing 6 2 6 4 win nicole vaidisova czech republic \\n\\nthe american much power 15 year old opponent\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(5):\n",
    "    print(\"Clean News #\",i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 29330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('C:\\\\Users\\\\beqaj\\\\Desktop\\\\NLP Lab\\\\project\\\\numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 134\n",
      "Percent of words that are missing from vocabulary: 0.45999999999999996%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 29330\n",
      "Number of words we will use: 22715\n",
      "Percent of words we will use: 77.45%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear â‰¥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22715\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 705050\n",
      "Total number of UNKs in headlines: 12017\n",
      "Percent of words that are UNK: 1.7000000000000002%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  16468.000000\n",
      "mean       4.266031\n",
      "std        2.584358\n",
      "min        0.000000\n",
      "25%        2.000000\n",
      "50%        4.000000\n",
      "75%        6.000000\n",
      "max       30.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  16468.000000\n",
      "mean      39.547304\n",
      "std       38.840707\n",
      "min        1.000000\n",
      "25%       17.000000\n",
      "50%       28.000000\n",
      "75%       48.000000\n",
      "max      783.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.0\n",
      "106.0\n",
      "194.32999999999993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "9.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11545\n",
      "11545\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove news that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "                                                                    _zero_state_tensors(rnn_size, \n",
    "                                                                                        batch_size, \n",
    "                                                                                        tf.float32)) \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input, \n",
    "                                                  summary_length, \n",
    "                                                  dec_cell, \n",
    "                                                  initial_state,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size, \n",
    "                                                  max_summary_length)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,  \n",
    "                                                    vocab_to_int['<GO>'], \n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell, \n",
    "                                                    initial_state, \n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 3 # use 100\n",
    "batch_size = 100\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    " # Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 2\n",
      "The longest text length: 51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Subset the data for training\n",
    "start = 1\n",
    "end = start + 10000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_summaries)\n",
    "len(sorted_texts)\n",
    "\n",
    "len(sorted_summaries_short)\n",
    "len(sorted_texts_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/3 Batch    3/166 - Loss:  9.865, Seconds: 5.67\n",
      "Epoch   1/3 Batch    6/166 - Loss:  4.818, Seconds: 5.54\n",
      "Epoch   1/3 Batch    9/166 - Loss:  3.885, Seconds: 6.88\n",
      "Epoch   1/3 Batch   12/166 - Loss:  3.672, Seconds: 5.44\n",
      "Epoch   1/3 Batch   15/166 - Loss:  2.684, Seconds: 5.02\n",
      "Epoch   1/3 Batch   18/166 - Loss:  3.637, Seconds: 5.82\n",
      "Epoch   1/3 Batch   21/166 - Loss:  3.642, Seconds: 5.45\n",
      "Epoch   1/3 Batch   24/166 - Loss:  2.930, Seconds: 6.35\n",
      "Epoch   1/3 Batch   27/166 - Loss:  2.861, Seconds: 6.92\n",
      "Epoch   1/3 Batch   30/166 - Loss:  3.020, Seconds: 5.55\n",
      "Epoch   1/3 Batch   33/166 - Loss:  3.274, Seconds: 5.60\n",
      "Epoch   1/3 Batch   36/166 - Loss:  3.111, Seconds: 5.12\n",
      "Epoch   1/3 Batch   39/166 - Loss:  2.427, Seconds: 6.18\n",
      "Epoch   1/3 Batch   42/166 - Loss:  3.099, Seconds: 6.88\n",
      "Epoch   1/3 Batch   45/166 - Loss:  2.663, Seconds: 5.59\n",
      "Epoch   1/3 Batch   48/166 - Loss:  2.778, Seconds: 5.81\n",
      "Epoch   1/3 Batch   51/166 - Loss:  2.629, Seconds: 6.95\n",
      "Epoch   1/3 Batch   54/166 - Loss:  2.481, Seconds: 6.35\n",
      "Average loss for this update: 3.527\n",
      "New Record!\n",
      "Epoch   1/3 Batch   57/166 - Loss:  3.078, Seconds: 7.01\n",
      "Epoch   1/3 Batch   60/166 - Loss:  2.933, Seconds: 5.19\n",
      "Epoch   1/3 Batch   63/166 - Loss:  2.604, Seconds: 8.30\n",
      "Epoch   1/3 Batch   66/166 - Loss:  2.561, Seconds: 6.52\n",
      "Epoch   1/3 Batch   69/166 - Loss:  3.397, Seconds: 7.52\n",
      "Epoch   1/3 Batch   72/166 - Loss:  2.640, Seconds: 7.73\n",
      "Epoch   1/3 Batch   75/166 - Loss:  3.641, Seconds: 5.85\n",
      "Epoch   1/3 Batch   78/166 - Loss:  2.937, Seconds: 6.91\n",
      "Epoch   1/3 Batch   81/166 - Loss:  2.760, Seconds: 7.67\n",
      "Epoch   1/3 Batch   84/166 - Loss:  2.763, Seconds: 8.18\n",
      "Epoch   1/3 Batch   87/166 - Loss:  2.637, Seconds: 6.63\n",
      "Epoch   1/3 Batch   90/166 - Loss:  2.855, Seconds: 8.66\n",
      "Epoch   1/3 Batch   93/166 - Loss:  2.972, Seconds: 7.11\n",
      "Epoch   1/3 Batch   96/166 - Loss:  2.876, Seconds: 7.13\n",
      "Epoch   1/3 Batch   99/166 - Loss:  2.864, Seconds: 6.96\n",
      "Epoch   1/3 Batch  102/166 - Loss:  2.806, Seconds: 8.91\n",
      "Epoch   1/3 Batch  105/166 - Loss:  2.454, Seconds: 8.40\n",
      "Epoch   1/3 Batch  108/166 - Loss:  2.923, Seconds: 6.45\n",
      "Average loss for this update: 2.872\n",
      "New Record!\n",
      "Epoch   1/3 Batch  111/166 - Loss:  2.830, Seconds: 6.87\n",
      "Epoch   1/3 Batch  114/166 - Loss:  2.904, Seconds: 7.90\n",
      "Epoch   1/3 Batch  117/166 - Loss:  2.531, Seconds: 9.45\n",
      "Epoch   1/3 Batch  120/166 - Loss:  2.624, Seconds: 8.87\n",
      "Epoch   1/3 Batch  123/166 - Loss:  3.238, Seconds: 6.73\n",
      "Epoch   1/3 Batch  126/166 - Loss:  3.237, Seconds: 7.87\n",
      "Epoch   1/3 Batch  129/166 - Loss:  2.969, Seconds: 9.60\n",
      "Epoch   1/3 Batch  132/166 - Loss:  2.879, Seconds: 8.37\n",
      "Epoch   1/3 Batch  135/166 - Loss:  2.680, Seconds: 9.60\n",
      "Epoch   1/3 Batch  138/166 - Loss:  2.600, Seconds: 10.36\n",
      "Epoch   1/3 Batch  141/166 - Loss:  2.754, Seconds: 8.67\n",
      "Epoch   1/3 Batch  144/166 - Loss:  2.800, Seconds: 8.76\n",
      "Epoch   1/3 Batch  147/166 - Loss:  2.557, Seconds: 10.03\n",
      "Epoch   1/3 Batch  150/166 - Loss:  3.073, Seconds: 9.16\n",
      "Epoch   1/3 Batch  153/166 - Loss:  2.869, Seconds: 9.12\n",
      "Epoch   1/3 Batch  156/166 - Loss:  3.080, Seconds: 9.29\n",
      "Epoch   1/3 Batch  159/166 - Loss:  2.938, Seconds: 9.32\n",
      "Epoch   1/3 Batch  162/166 - Loss:  3.115, Seconds: 11.39\n",
      "Average loss for this update: 2.871\n",
      "New Record!\n",
      "Epoch   1/3 Batch  165/166 - Loss:  2.926, Seconds: 10.23\n",
      "Epoch   2/3 Batch    3/166 - Loss:  6.071, Seconds: 5.11\n",
      "Epoch   2/3 Batch    6/166 - Loss:  3.258, Seconds: 5.18\n",
      "Epoch   2/3 Batch    9/166 - Loss:  2.971, Seconds: 6.44\n",
      "Epoch   2/3 Batch   12/166 - Loss:  3.002, Seconds: 4.83\n",
      "Epoch   2/3 Batch   15/166 - Loss:  2.190, Seconds: 4.86\n",
      "Epoch   2/3 Batch   18/166 - Loss:  3.034, Seconds: 5.46\n",
      "Epoch   2/3 Batch   21/166 - Loss:  3.017, Seconds: 4.96\n",
      "Epoch   2/3 Batch   24/166 - Loss:  2.544, Seconds: 5.98\n",
      "Epoch   2/3 Batch   27/166 - Loss:  2.487, Seconds: 5.99\n",
      "Epoch   2/3 Batch   30/166 - Loss:  2.608, Seconds: 5.78\n",
      "Epoch   2/3 Batch   33/166 - Loss:  2.848, Seconds: 6.40\n",
      "Epoch   2/3 Batch   36/166 - Loss:  2.707, Seconds: 7.77\n",
      "Epoch   2/3 Batch   39/166 - Loss:  2.165, Seconds: 7.10\n",
      "Epoch   2/3 Batch   42/166 - Loss:  2.741, Seconds: 8.13\n",
      "Epoch   2/3 Batch   45/166 - Loss:  2.386, Seconds: 5.74\n",
      "Epoch   2/3 Batch   48/166 - Loss:  2.481, Seconds: 5.97\n",
      "Epoch   2/3 Batch   51/166 - Loss:  2.315, Seconds: 7.15\n",
      "Epoch   2/3 Batch   54/166 - Loss:  2.194, Seconds: 6.47\n",
      "Average loss for this update: 2.834\n",
      "New Record!\n",
      "Epoch   2/3 Batch   57/166 - Loss:  2.772, Seconds: 6.49\n",
      "Epoch   2/3 Batch   60/166 - Loss:  2.636, Seconds: 5.05\n",
      "Epoch   2/3 Batch   63/166 - Loss:  2.355, Seconds: 8.00\n",
      "Epoch   2/3 Batch   66/166 - Loss:  2.302, Seconds: 6.73\n",
      "Epoch   2/3 Batch   69/166 - Loss:  3.004, Seconds: 7.15\n",
      "Epoch   2/3 Batch   72/166 - Loss:  2.382, Seconds: 7.70\n",
      "Epoch   2/3 Batch   75/166 - Loss:  3.190, Seconds: 6.21\n",
      "Epoch   2/3 Batch   78/166 - Loss:  2.588, Seconds: 6.82\n",
      "Epoch   2/3 Batch   81/166 - Loss:  2.493, Seconds: 6.77\n",
      "Epoch   2/3 Batch   84/166 - Loss:  2.522, Seconds: 7.39\n",
      "Epoch   2/3 Batch   87/166 - Loss:  2.416, Seconds: 5.75\n",
      "Epoch   2/3 Batch   90/166 - Loss:  2.597, Seconds: 6.94\n",
      "Epoch   2/3 Batch   93/166 - Loss:  2.647, Seconds: 6.97\n",
      "Epoch   2/3 Batch   96/166 - Loss:  2.618, Seconds: 6.52\n",
      "Epoch   2/3 Batch   99/166 - Loss:  2.612, Seconds: 6.50\n",
      "Epoch   2/3 Batch  102/166 - Loss:  2.580, Seconds: 8.74\n",
      "Epoch   2/3 Batch  105/166 - Loss:  2.281, Seconds: 8.88\n",
      "Epoch   2/3 Batch  108/166 - Loss:  2.654, Seconds: 6.58\n",
      "Average loss for this update: 2.591\n",
      "New Record!\n",
      "Epoch   2/3 Batch  111/166 - Loss:  2.581, Seconds: 7.03\n",
      "Epoch   2/3 Batch  114/166 - Loss:  2.638, Seconds: 7.38\n",
      "Epoch   2/3 Batch  117/166 - Loss:  2.324, Seconds: 9.17\n",
      "Epoch   2/3 Batch  120/166 - Loss:  2.390, Seconds: 7.99\n",
      "Epoch   2/3 Batch  123/166 - Loss:  2.931, Seconds: 7.09\n",
      "Epoch   2/3 Batch  126/166 - Loss:  2.930, Seconds: 7.96\n",
      "Epoch   2/3 Batch  129/166 - Loss:  2.714, Seconds: 8.89\n",
      "Epoch   2/3 Batch  132/166 - Loss:  2.654, Seconds: 7.80\n",
      "Epoch   2/3 Batch  135/166 - Loss:  2.443, Seconds: 10.23\n",
      "Epoch   2/3 Batch  138/166 - Loss:  2.391, Seconds: 9.90\n",
      "Epoch   2/3 Batch  141/166 - Loss:  2.542, Seconds: 8.56\n",
      "Epoch   2/3 Batch  144/166 - Loss:  2.556, Seconds: 8.21\n",
      "Epoch   2/3 Batch  147/166 - Loss:  2.355, Seconds: 9.48\n",
      "Epoch   2/3 Batch  150/166 - Loss:  2.826, Seconds: 9.63\n",
      "Epoch   2/3 Batch  153/166 - Loss:  2.668, Seconds: 9.15\n",
      "Epoch   2/3 Batch  156/166 - Loss:  2.838, Seconds: 9.33\n",
      "Epoch   2/3 Batch  159/166 - Loss:  2.740, Seconds: 12.70\n",
      "Epoch   2/3 Batch  162/166 - Loss:  2.863, Seconds: 12.94\n",
      "Average loss for this update: 2.632\n",
      "No Improvement.\n",
      "Epoch   2/3 Batch  165/166 - Loss:  2.858, Seconds: 10.72\n",
      "Epoch   3/3 Batch    3/166 - Loss:  5.425, Seconds: 5.17\n",
      "Epoch   3/3 Batch    6/166 - Loss:  3.070, Seconds: 5.26\n",
      "Epoch   3/3 Batch    9/166 - Loss:  2.926, Seconds: 6.00\n",
      "Epoch   3/3 Batch   12/166 - Loss:  2.739, Seconds: 5.36\n",
      "Epoch   3/3 Batch   15/166 - Loss:  2.107, Seconds: 5.61\n",
      "Epoch   3/3 Batch   18/166 - Loss:  2.843, Seconds: 6.41\n",
      "Epoch   3/3 Batch   21/166 - Loss:  2.799, Seconds: 6.52\n",
      "Epoch   3/3 Batch   24/166 - Loss:  2.356, Seconds: 7.32\n",
      "Epoch   3/3 Batch   27/166 - Loss:  2.325, Seconds: 6.10\n",
      "Epoch   3/3 Batch   30/166 - Loss:  2.508, Seconds: 5.64\n",
      "Epoch   3/3 Batch   33/166 - Loss:  2.690, Seconds: 5.79\n",
      "Epoch   3/3 Batch   36/166 - Loss:  2.548, Seconds: 5.33\n",
      "Epoch   3/3 Batch   39/166 - Loss:  2.005, Seconds: 6.86\n",
      "Epoch   3/3 Batch   42/166 - Loss:  2.556, Seconds: 6.62\n",
      "Epoch   3/3 Batch   45/166 - Loss:  2.243, Seconds: 5.46\n",
      "Epoch   3/3 Batch   48/166 - Loss:  2.348, Seconds: 6.98\n",
      "Epoch   3/3 Batch   51/166 - Loss:  2.200, Seconds: 7.23\n",
      "Epoch   3/3 Batch   54/166 - Loss:  2.086, Seconds: 6.96\n",
      "Average loss for this update: 2.654\n",
      "No Improvement.\n",
      "Epoch   3/3 Batch   57/166 - Loss:  2.582, Seconds: 6.66\n",
      "Epoch   3/3 Batch   60/166 - Loss:  2.460, Seconds: 6.72\n",
      "Epoch   3/3 Batch   63/166 - Loss:  2.202, Seconds: 9.89\n",
      "Epoch   3/3 Batch   66/166 - Loss:  2.149, Seconds: 7.71\n",
      "Epoch   3/3 Batch   69/166 - Loss:  2.805, Seconds: 7.78\n",
      "Epoch   3/3 Batch   72/166 - Loss:  2.256, Seconds: 9.23\n",
      "Epoch   3/3 Batch   75/166 - Loss:  2.929, Seconds: 5.84\n",
      "Epoch   3/3 Batch   78/166 - Loss:  2.371, Seconds: 7.17\n",
      "Epoch   3/3 Batch   81/166 - Loss:  2.330, Seconds: 7.56\n",
      "Epoch   3/3 Batch   84/166 - Loss:  2.373, Seconds: 9.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/3 Batch   87/166 - Loss:  2.296, Seconds: 5.87\n",
      "Epoch   3/3 Batch   90/166 - Loss:  2.447, Seconds: 7.07\n",
      "Epoch   3/3 Batch   93/166 - Loss:  2.498, Seconds: 7.66\n",
      "Epoch   3/3 Batch   96/166 - Loss:  2.427, Seconds: 7.24\n",
      "Epoch   3/3 Batch   99/166 - Loss:  2.438, Seconds: 6.58\n",
      "Epoch   3/3 Batch  102/166 - Loss:  2.417, Seconds: 8.81\n",
      "Epoch   3/3 Batch  105/166 - Loss:  2.145, Seconds: 8.44\n",
      "Epoch   3/3 Batch  108/166 - Loss:  2.490, Seconds: 6.15\n",
      "Average loss for this update: 2.423\n",
      "New Record!\n",
      "Epoch   3/3 Batch  111/166 - Loss:  2.435, Seconds: 6.89\n",
      "Epoch   3/3 Batch  114/166 - Loss:  2.485, Seconds: 8.34\n",
      "Epoch   3/3 Batch  117/166 - Loss:  2.181, Seconds: 9.06\n",
      "Epoch   3/3 Batch  120/166 - Loss:  2.261, Seconds: 8.64\n",
      "Epoch   3/3 Batch  123/166 - Loss:  2.722, Seconds: 7.13\n",
      "Epoch   3/3 Batch  126/166 - Loss:  2.731, Seconds: 7.84\n",
      "Epoch   3/3 Batch  129/166 - Loss:  2.530, Seconds: 9.07\n",
      "Epoch   3/3 Batch  132/166 - Loss:  2.489, Seconds: 7.93\n",
      "Epoch   3/3 Batch  135/166 - Loss:  2.263, Seconds: 10.38\n",
      "Epoch   3/3 Batch  138/166 - Loss:  2.231, Seconds: 10.19\n",
      "Epoch   3/3 Batch  141/166 - Loss:  2.357, Seconds: 9.36\n",
      "Epoch   3/3 Batch  144/166 - Loss:  2.364, Seconds: 8.97\n",
      "Epoch   3/3 Batch  147/166 - Loss:  2.231, Seconds: 11.32\n",
      "Epoch   3/3 Batch  150/166 - Loss:  2.652, Seconds: 10.03\n",
      "Epoch   3/3 Batch  153/166 - Loss:  2.463, Seconds: 9.35\n",
      "Epoch   3/3 Batch  156/166 - Loss:  2.662, Seconds: 9.99\n",
      "Epoch   3/3 Batch  159/166 - Loss:  2.542, Seconds: 10.18\n",
      "Epoch   3/3 Batch  162/166 - Loss:  2.697, Seconds: 11.14\n",
      "Average loss for this update: 2.461\n",
      "No Improvement.\n",
      "Epoch   3/3 Batch  165/166 - Loss:  2.569, Seconds: 10.71\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"C:\\\\Users\\\\beqaj\\\\Desktop\\\\NLP Lab\\\\project\\\\best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "\n",
      "\n",
      "\n",
      "Original Text: england coach andy robinson insisted livid side denied two tries sunday\\ 19 13 six nations loss ireland dublin \\n\\nmark cueto\\ first half effort ruled offside referee spurned tv replays england crashed dying minutes i\\ absolutely spitting i\\ livid there\\ two tries we\\ cost\n",
      "\n",
      "Summary\n",
      "\n",
      "Summary : fuming robinson blasts officials\n"
     ]
    }
   ],
   "source": [
    "#text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "\n",
    "    \n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('\\n\\n')\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nSummary')\n",
    "print('\\nSummary :' ,  clean_summaries[random])\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenserflow",
   "language": "python",
   "name": "tenserflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
