{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "# Read training files\n",
    "reviewsTrain = load_files(\"C:/Users/Sadie/Desktop/project/bbc/train\")\n",
    "# Lets get training reviews and training labels in sepearate lists\n",
    "X_train, y_train = reviewsTrain.data, reviewsTrain.target\n",
    "\n",
    "\n",
    "reviewsTest = load_files(\"C:/Users/Sadie/Desktop/project/bbc/train\")\n",
    "X_test, y_test = reviewsTest.data, reviewsTest.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textTrain = []\n",
    "for each in X_train:\n",
    "    x = each.decode(\"UTF-8\", errors = 'ignore')\n",
    "    textTrain.append(x)\n",
    "    \n",
    "labelsTrain = np.array(y_train)\n",
    "labelsTrain_one_hot = pd.get_dummies(labelsTrain)\n",
    "\n",
    "textTest = []\n",
    "for each in X_test:\n",
    "    y = each.decode(\"UTF-8\", errors = 'ignore')\n",
    "    textTest.append(y)\n",
    "    \n",
    "labelsTest = np.array(y_test)\n",
    "labelsTest_one_hot = pd.get_dummies(labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize, stem, remove stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "#word_tokenize accepts a string as an input, not a file.\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for doc in range(len(textTrain)):\n",
    "    clean_text_train = []\n",
    "    text = (textTrain[doc])\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            clean_text_train.append((token))\n",
    "    news = \" \".join(clean_text_train)\n",
    "    textTrain[doc] = news\n",
    "    \n",
    "for doc in range(len(textTest)):\n",
    "    clean_text_test = []\n",
    "    text = (textTest[doc])\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    for token in tokens:\n",
    "        if not token in stop_words:\n",
    "            clean_text_test.append((token))\n",
    "    news = \" \".join(clean_text_test)\n",
    "    textTest[doc] = news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = 10000\n",
    "t = Tokenizer(num_words = top_words)\n",
    "t.fit_on_texts(textTrain)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "#print(vocab_size)\n",
    "encoded_docs = t.texts_to_sequences(textTrain)\n",
    "#print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 3000\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "embedding_vecor_length = 300\n",
    "#print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All above steps are same as RNN in order to compare the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN Model part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of words we want our convolutional filters to cover\n",
    "filter_sizes = [1,2]\n",
    "num_filters = 32 \n",
    "#always default set 32 or 64\n",
    "#we will have filters that slide over 1 and 2 words respectively, for a total of 2 * num_filters filters\n",
    "def get_model():    \n",
    "    inp = Input(shape=(max_length, ))\n",
    "    x = Embedding(top_words, embedding_vecor_length, input_length=max_length)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Reshape((max_length, embedding_vecor_length, 1))(x)\n",
    "#embedding layer followed by a drop out layer with rate0.1\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_vecor_length), kernel_initializer='normal',activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_vecor_length), kernel_initializer='normal',activation='relu')(x)\n",
    "#convolution layers    \n",
    "    maxpool_0 = MaxPool2D(pool_size=(max_length - filter_sizes[0] + 1, 1))(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(max_length - filter_sizes[1] + 1, 1))(conv_1)\n",
    "#max pooling layers \n",
    "    z = Concatenate(axis=1)([maxpool_0, maxpool_1])\n",
    "#merge the max pooling layers\n",
    "\n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "        \n",
    "    outp = Dense(5, activation=\"sigmoid\")(z)\n",
    "#The last stage of a convolutional neural network (CNN) is a classifier. It is called a dense layer, which is just an \n",
    "#artificial neural network (ANN) classifier.\n",
    "#And an ANN classifier needs individual features, just like any other classifier. This means it needs a feature vector.\n",
    "#Therefore, you need to convert the output of the convolutional part of the CNN into a 1D feature vector, \n",
    "#to be used by the ANN part of it. This operation is called flattening. It gets the output of the convolutional layers, \n",
    "#flattens all its structure to create a single long feature vector to be used by the dense layer for the final\n",
    "#classification. # https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network\n",
    "\n",
    "#traning model    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 3000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 3000, 300)    3000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 3000, 300)    0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3000, 300, 1) 0           spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 3000, 1, 32)  9632        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 2999, 1, 32)  19232       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 32)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 1, 32)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 64)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            325         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,029,189\n",
      "Trainable params: 3,029,189\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1584/1584 [==============================] - 101s 64ms/step - loss: 0.1088 - acc: 0.9705\n",
      "Epoch 2/3\n",
      "1584/1584 [==============================] - 99s 63ms/step - loss: 0.0648 - acc: 0.9837\n",
      "Epoch 3/3\n",
      "1584/1584 [==============================] - 125s 79ms/step - loss: 0.0440 - acc: 0.9903\n",
      "0:05:26\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "start_time = datetime.datetime.now().time().strftime('%H:%M:%S')\n",
    "model.fit(padded_docs, labelsTrain_one_hot, epochs=3)\n",
    "end_time = datetime.datetime.now().time().strftime('%H:%M:%S')\n",
    "total_time=(datetime.datetime.strptime(end_time,'%H:%M:%S') - datetime.datetime.strptime(start_time,'%H:%M:%S'))\n",
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting test dataset into integers and then padding\n",
    "top_words = 10000\n",
    "t = Tokenizer(num_words = top_words, lower = True)\n",
    "t.fit_on_texts(textTest)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "#print(vocab_size)\n",
    "encoded_docs = t.texts_to_sequences(textTest)\n",
    "\n",
    "max_length = 3000\n",
    "padded_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.71%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(padded_test, labelsTest_one_hot, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenserflow",
   "language": "python",
   "name": "tenserflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
